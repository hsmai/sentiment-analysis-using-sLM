{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install google-generativeai datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tUeyh3jR8YV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51566730-5c72-4a71-ac1b-f839b2454167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-7zc51cni/unsloth_775e780b6de14c29971aa1d38136bdf6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-7zc51cni/unsloth_775e780b6de14c29971aa1d38136bdf6\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 2eb6b0d5f363a60ed3792ea1f04250537ac66939\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unsloth_zoo>=2025.12.6 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading unsloth_zoo-2025.12.6-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
            "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading tyro-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.57.3)\n",
            "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.36.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
            "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.0)\n",
            "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.0)\n",
            "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.12.0)\n",
            "Collecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.18.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.12.6->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.11.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.12.6-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-1.0.3-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.12.8-py3-none-any.whl size=382385 sha256=c691ca04fff629c525bd81e910eea667ab02f939656160446b184405b4a6f566\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-evkubun6/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
            "Successfully built unsloth\n",
            "Installing collected packages: torchao, unsloth, pyarrow, msgspec, tyro, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed bitsandbytes-0.49.0 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.20.0 pyarrow-22.0.0 torchao-0.15.0 trl-0.24.0 tyro-1.0.3 unsloth-2025.12.8 unsloth_zoo-2025.12.6\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting trl<0.9.0\n",
            "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m430.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, trl\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.24.0\n",
            "    Uninstalling trl-0.24.0:\n",
            "      Successfully uninstalled trl-0.24.0\n",
            "Successfully installed trl-0.8.6 xformers-0.0.33.post2\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë°ì´í„° ë° ëª¨ë¸ ì„¤ì •"
      ],
      "metadata": {
        "id": "Qhb5ixktFi7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
        "dataset_name = \"Blpeng/nsmc\"\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ & ìƒ˜í”Œë§\n",
        "dataset = load_dataset(dataset_name)\n",
        "sampled_train = dataset[\"train\"].shuffle(3407).select(range(1000))\n",
        "sampled_test  = dataset[\"test\" ].shuffle(3407).select(range(1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250,
          "referenced_widgets": [
            "3656eb6f3e214cf791c26bb7bfa8157f",
            "f87eece518fe41a997e1a1300aa4df6a",
            "c2309e8f9b6d4c9d87502df02cbf0cc2",
            "4e2212474f98404ba74aa0bbc4a67e2b",
            "0f07f6908e564e988216b3d4c54423f2",
            "365af2fbb65e4d5a91f5db3055ea7423",
            "cc060d037626420d936be918efc66590",
            "679bd684be674d1a966aae25ba05b9d2",
            "733515893c134df8aed8539623e55add",
            "8fe2ba1d0b1b456999e3e44bc61a8bd5",
            "f71d909754af4c659bbb512b4a023288",
            "e0457a42bdba48d7a798970123090a96",
            "4fb7f65ab4fe4847a231ec102f5dbd9d",
            "e92a62a948cd47fb8c30ae171f9ec811",
            "63173330be6c4c2ca85f59ea71eab13d",
            "f320aae8e688420fb37237d02e9fdef7",
            "abdf85e7b9b444239aeea8eb006ab524",
            "94a8a2e21c3247e88082640adbe532c0",
            "2f8a17bf08e94922abf7c758f5f74122",
            "fcf9d27621ff489e860eb7777fd3fff9",
            "a714245e857c472bb054e92360861a43",
            "ff0cbc6e9c244c7ea5152ba99ec4d763",
            "59caa3b618424bad8946a80b6de712a1",
            "6178ba97838f421fae2041786a218ce1",
            "f60ff18e51bb4f638e17fa1096282b48",
            "a38a48324f3c47afaa11816d1e580bdb",
            "c6ff610e41b14336b64da9704ffd55a7",
            "af52036e117c4c658b9e6e039b75cabf",
            "47b54c9cbbae42ac9cf028640cb76cf1",
            "01e6789a3b624c70b0873e796d967fc4",
            "0355bc0b258e43658ebe638a832d4b57",
            "42927d4c5afe46b8be1492347aca289d",
            "066c55496a3242a2bea5cff1c74b8e76",
            "d5d2d707e9cc4837af9cfcce480ae7c5",
            "d5e7cb7a04364768a8abb9a7777c5300",
            "d96ee5655a39459c9b2eb9ef0124569b",
            "0cd88ac6c42f45b69b94e7da3ab26975",
            "a91fc31a9d45431f8e0c3e829c9876fb",
            "6a78cd1e3b5f4f2292409d5767f19887",
            "f3e5cb15d0174048bcb736d1c98806b0",
            "9fd1304f249f4095a75dc4702419913c",
            "6b6af3449ce94d0388124d78826a9323",
            "764dcb5b3a3940ef9299b2feeea22c33",
            "5ef1b32aa41146d3996120d07fdca096",
            "ae906dc1d20d493cb367a37a1eddf0b1",
            "21ba54202cfd4eb1b0d30aea3b7b01ba",
            "1458226808104e7cbb18c88ec66576f8",
            "9b4fda9ac4ed434b96c2be23c646e3d1",
            "282ca014a14c4706845428bfa2eb05da",
            "05e96e8696fd493598d8bf1a9d5b1216",
            "7fb4796f530f41b7a7adb9a3afdf290a",
            "2ed67cdc56a049469008f00a94ce7204",
            "1193b33c4bae42a1aff8cec513f82c48",
            "dde41ce5787140b5a7df0a7cbe6bb76b",
            "cb70712faaad4674b74aafa337ca92d4"
          ]
        },
        "id": "V6lbLRJ2KiSF",
        "outputId": "0342f5c0-93ef-41d1-95ed-650c5c1cf21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/34.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3656eb6f3e214cf791c26bb7bfa8157f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ratings_train.csv:   0%|          | 0.00/15.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0457a42bdba48d7a798970123090a96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ratings_test.csv: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59caa3b618424bad8946a80b6de712a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/150000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5d2d707e9cc4837af9cfcce480ae7c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae906dc1d20d493cb367a37a1eddf0b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(\"train columns:\", dataset[\"train\"].column_names)\n",
        "print(\"test columns:\", dataset[\"test\"].column_names)\n",
        "print(\"train sample:\", dataset[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN-Qki44Kugx",
        "outputId": "79906a9e-8260-4ffd-ed38-271b51e9aba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
            "        num_rows: 150000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "train columns: ['Unnamed: 0', 'id', 'document', 'label']\n",
            "test columns: ['Unnamed: 0', 'id', 'document', 'label']\n",
            "train sample: {'Unnamed: 0': 0, 'id': 9976970, 'document': 'ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½"
      ],
      "metadata": {
        "id": "mKN_0NtcK4_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í”„ë¡¬í”„íŠ¸/ë¼ë²¨\n",
        "prompt = \"\"\"ë‹¤ìŒì€ ì˜í™” ë¦¬ë·°ì…ë‹ˆë‹¤:\n",
        "{}\n",
        "\n",
        "ì´ ë¦¬ë·°ì˜ ê°ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”? \"ê¸ì •\" ë˜ëŠ” \"ë¶€ì •\"ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”.\n",
        "\n",
        "ì •ë‹µ\n",
        "ì •ë‹µì€: \"{}\"\"\"\n",
        "\n",
        "positivelabel = \"ê¸ì •\"\n",
        "negativelabel = \"ë¶€ì •\""
      ],
      "metadata": {
        "id": "QRMBXkSfKiPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(dataset_):\n",
        "    if isinstance(dataset_[\"document\"], str):\n",
        "        t = dataset_[\"document\"]\n",
        "        y = dataset_[\"label\"]\n",
        "        label = positivelabel if y == 1 else negativelabel\n",
        "        return [prompt.format(t, label)]\n",
        "\n",
        "    texts = []\n",
        "    for t, y in zip(dataset_[\"document\"], dataset_[\"label\"]):\n",
        "        label = positivelabel if y == 1 else negativelabel\n",
        "        texts.append(prompt.format(t, label))\n",
        "    return texts"
      ],
      "metadata": {
        "id": "IpmxAuz7KiMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ í•¨ìˆ˜\n",
        "def test_model(model, tokenizer, test_dataset, pos_token_id, neg_token_id):\n",
        "    tokenized_inputs = []\n",
        "    for i in tqdm(range(len(test_dataset[\"document\"]))):\n",
        "        text = test_dataset[\"document\"][i]\n",
        "        test_str = prompt.format(text, \"\")\n",
        "        tokenized_input = tokenizer(test_str, return_tensors=\"pt\", add_special_tokens=False)\n",
        "        tokenized_inputs.append((tokenized_input, test_dataset[\"label\"][i]))\n",
        "\n",
        "    tokenized_inputs.sort(key=lambda x: x[0][\"input_ids\"].shape[1])\n",
        "\n",
        "    grouped_inputs = defaultdict(list)\n",
        "    for tokenized_input, label in tokenized_inputs:\n",
        "        length = tokenized_input[\"input_ids\"].shape[1]\n",
        "        grouped_inputs[length].append((tokenized_input, label))\n",
        "\n",
        "    batch_size = 32\n",
        "    all_outputs, all_labels = [], []\n",
        "\n",
        "    for length, group in tqdm(grouped_inputs.items()):\n",
        "        for i in range(0, len(group), batch_size):\n",
        "            batch = group[i:i+batch_size]\n",
        "            batch_inputs = [item[0] for item in batch]\n",
        "            batch_labels = [item[1] for item in batch]\n",
        "\n",
        "            input_ids = torch.cat([item[\"input_ids\"] for item in batch_inputs], dim=0).to(\"cuda\")\n",
        "            attention_mask = torch.cat([item[\"attention_mask\"] for item in batch_inputs], dim=0).to(\"cuda\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            last_token_logits = outputs.logits[:, -1, :]  # ë§ˆì§€ë§‰ í† í°\n",
        "            neg_pos_logits = last_token_logits[:, [neg_token_id, pos_token_id]]  # [neg, pos]\n",
        "            probs = torch.softmax(neg_pos_logits, dim=-1)\n",
        "            preds = torch.argmax(probs, dim=-1)  # 0=neg, 1=pos\n",
        "\n",
        "            all_outputs.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(batch_labels)\n",
        "\n",
        "    correct = sum(int(p) == int(y) for p, y in zip(all_outputs, all_labels))\n",
        "    total = len(all_outputs)\n",
        "    acc = correct / total\n",
        "    print(f\"Correct: {correct} Total: {total} Accuracy: {acc:.4f}\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "FhQnse8dKiJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE (no fine-tuning)"
      ],
      "metadata": {
        "id": "WqjVGKAsK9g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline: LoRA ì—†ì´ ì›ë³¸ ëª¨ë¸ ë¡œë“œ\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        ")\n",
        "\n",
        "# ê¸ì •/ë¶€ì • í† í° id\n",
        "pos_token_id = tokenizer.encode(\"ê¸ì •\", add_special_tokens=False)[0]\n",
        "neg_token_id = tokenizer.encode(\"ë¶€ì •\", add_special_tokens=False)[0]\n",
        "\n",
        "print(\"=== Baseline (no fine-tuning) ===\")\n",
        "baseline_acc = test_model(base_model, tokenizer, sampled_test, pos_token_id, neg_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "366850638b5a45048d18ebd7388b99f3",
            "a6b988c207564b5c91674f6e495763dd",
            "a9e6d15729874d0b85ef6fb0c6bdd1e5",
            "a7d1198fb4bd4561b4f0ddf1912684d9",
            "0fcb6c45c4174e19b8042d4fa9a91915",
            "226d1382bea24709abda55c66418ae5a",
            "4430a6b8e48447fd84f3fa8037593a9d",
            "cf052d24ad2b4fb88d322b73d80ba421",
            "7ac8db63269545c09c847eb8ebb8ac7e",
            "050f1a54483a4deaa12e70b0878a1808",
            "2181388c6cce45f1a56f6d47c7d4b069",
            "e49cd4f9b01640acb189a11c6e05b829",
            "9a668e38840c4f498c8bc326f86d36ed",
            "94ebe0f37bcb4cd7add6ecfa21ea6890",
            "a7a8a648e48c4493ad13402d5b9c8c47",
            "5de004266c99499ca79923febc3994bf",
            "0efa01d2c5f34514bf063aa3f8ebf800",
            "5e8a8b01590b446ea186174586b20fde",
            "3607227e1b4e4c60a69dcba1ab72797b",
            "257fcf0f3d5045f3b8aa4d67f2b481ab",
            "361cb330a42f4955afa90f273ef58cb6",
            "eaa5e160f78c4aec82cf3856a3c4eb1e",
            "eb7a31d167e14012b0ded0956569b204",
            "21411458bcae4543bde8757a1d43ade0",
            "e41d9096323b4a79a72b79ec81f5c14c",
            "67d28b1011814569843c04ffdb44dd8e",
            "b59eeec6d0f94628bf94043b8a5dd725",
            "49ac20606040475785c3ffa9ba3bb797",
            "57acbc9f034446fb920af549143a38d5",
            "aa2d8b2ad43d466d8d7d7b18f98f8631",
            "2b2588ab90914bcdad98861954feac18",
            "2f97e4b9a36348299d99f3d1c10ba5a7",
            "49d3002e7d294aafbea7697c19639e9c",
            "071e59b299a0416aa0b9abc2e66918c8",
            "1695e0e3f4594c7ab448fac81911db5b",
            "6f851bf2ffdd49b58ce0592610e35749",
            "a0223ab19f454477b096d2065bc60ebb",
            "666a48cf5ca442d295b331e4845c3147",
            "4b3b69feafce47e4953a5761393f6461",
            "a1a1c36aa0c5421e9ed9b6eab9bd3e8b",
            "004e8079340f4045938482416a3b310a",
            "cf6545819d634e899c8f0c487279dbe8",
            "1ca815fa5ff2414f9987bfd79d8e4bd5",
            "c42eaaa74623499aa89eb4973266b866",
            "52832b7510dc47e9a8e52dabc36e2391",
            "43bde26a855d4aaeb09dea25e6c2c89e",
            "6194f23d961947aaadab4a3a4e0df250",
            "2676bfd900f34e228130d7d41bd1d09d",
            "adafde1ca8fb40389c894d14b4016a9e",
            "76d518b6d07644b99b278140102be4bd",
            "5d39db2f02f544e59addc9105820db83",
            "7312103925664ae696728c1521c26716",
            "6e08b0510ac149d6a721fab3dbd15122",
            "a5605c36e2624fd9b5d689295ec69e92",
            "56a045b893fc46e0bb090c1f41cca386",
            "d7ea5adb4a014b9abf2cfbca7d713bdc",
            "ceee9595d21f474c812696f51d28f119",
            "c8018f8357da49b2af43b6a2ef7bc710",
            "cc16adb9caa5454d9ed31e22098d850d",
            "8570d5a8132e4a48b30f7e4b43ca33d3",
            "e72d6db50df647debb40cef0dc5a8323",
            "fe88e6da16d74e4f91203897b77e1922",
            "242dc03d4c6d4fe18efc46734f87378e",
            "d3675f7db1b343b9961ced15fe02ce8d",
            "66a9dae8907d49aa896f306d0e72166d",
            "d06d43d06c3f4ea8b7ea6869267cdb54",
            "6389a1f69f3c455596a86fd5f9eb0750",
            "a3094ffa3ce045188ef175482e568cb8",
            "9073fb7ad0d34744a785d0dc7db216e1",
            "c38aa42fdd494d31a9a08c00cc0ca89c",
            "de865ef78fd94982b6ab74c6a6366b13",
            "2453698289bd432190686da72d134ff1",
            "8d1210b4801a45978e99f866f173d99a",
            "48896aaa4a9446faac6aa1a048994a0f",
            "cb55709c99b64f7490a8a194f6a61084",
            "88b015381e3a413ba2a0b9a2503aa1ca",
            "db0a77eb65784283967ca227cde7dc01",
            "68e2746d8c26481aa626a1eb74dd423a",
            "4bbbe5fc7c61403aafee90d5cff9c252",
            "12ad61b3e47f489ea5bab0e7a13cfd01",
            "0cc46a005908459593d70352b583104e",
            "966068f0788548d487cea2b3e15debe3",
            "40047d308a704de9acf2ce083600a235",
            "fb254fc233ae4ef58c5fe6c75a56c8c7",
            "2f147d444e3d4dd1acba35cfbbeae500",
            "7ca9c9485588411f88fb9daa9614daab",
            "db50a968eef4424698b9c55191fb42b0",
            "ab939b69779e43df8f845ec13897e05b"
          ]
        },
        "id": "IuFE7PNyKiHD",
        "outputId": "33ffdd87-c54e-4aa7-b332-47e6ab953c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.8: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "366850638b5a45048d18ebd7388b99f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e49cd4f9b01640acb189a11c6e05b829"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb7a31d167e14012b0ded0956569b204"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "071e59b299a0416aa0b9abc2e66918c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52832b7510dc47e9a8e52dabc36e2391"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7ea5adb4a014b9abf2cfbca7d713bdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6389a1f69f3c455596a86fd5f9eb0750"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68e2746d8c26481aa626a1eb74dd423a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bllossom/llama-3.2-Korean-Bllossom-3B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
            "=== Baseline (no fine-tuning) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 793.27it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 698 Total: 1000 Accuracy: 0.6980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA FINE-TUNING (r=4)"
      ],
      "metadata": {
        "id": "S-BBJ-igLGUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# r=4: base model ìƒˆë¡œ ë¡œë“œ\n",
        "model_r4, tokenizer_r4 = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "f31d68f3124d41d7a60c33ed537e7e06",
            "32c593c970d64d8ea75ffacb46b28461",
            "9d422f9a3a5b4891b5039af85a1f58dd",
            "de4fc4bd9bb640b183f3679cb3f56514",
            "3ebaa8109bfc41c4ae18bd8c92d57c18",
            "196024b2ea0f4e5ba38db28aa97976eb",
            "124c7f7b5f7a4f1ab004ae464530254d",
            "b86b3d53b3c244d59882d09873624e89",
            "ba2226a7537c46b8afed68e05d75f808",
            "c1db07e5e46b4dca8b4e4b8bd6ac3c30",
            "aa2cb1f70a8144bbb7764d33960976f9"
          ]
        },
        "id": "5YpA15ZlKiEC",
        "outputId": "3e35c206-977f-44de-b9f8-00f44fdc2ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.8: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f31d68f3124d41d7a60c33ed537e7e06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bllossom/llama-3.2-Korean-Bllossom-3B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_token_id_r4 = tokenizer_r4.encode(\"ê¸ì •\", add_special_tokens=False)[0]\n",
        "neg_token_id_r4 = tokenizer_r4.encode(\"ë¶€ì •\", add_special_tokens=False)[0]"
      ],
      "metadata": {
        "id": "RPqUgxPLKiBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "peft_r4 = FastLanguageModel.get_peft_model(\n",
        "    model_r4,\n",
        "    r=4,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6DNpDH1Kh-J",
        "outputId": "4bbcec5d-486c-4924-aec9-1e57ce5d26ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Unsloth 2025.12.8 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "trainer_r4 = SFTTrainer(\n",
        "    model=peft_r4,\n",
        "    tokenizer=tokenizer_r4,\n",
        "    train_dataset=sampled_train,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_r4\",\n",
        "        num_train_epochs=1,\n",
        "        report_to=\"none\",\n",
        "        group_by_length=False,\n",
        "    ),\n",
        "    formatting_func=formatting_prompts_func,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e76231822f26444aab07bf685694ce92",
            "b0992a0b92484ab8bdf69091ce8db16a",
            "088dafd7d0b54665a1fa4b4e132e686c",
            "63b007ff70d449e79ad8273d35fbf7ac",
            "674b6b5ed4a347dd99e30f6fcbbe80ff",
            "adf17ed6316042d3945eb1878fe735b1",
            "9079b8515f0e47108cde435c813a397b",
            "1c3b804f76b149db90af92ce70d877f0",
            "5326e9d41efa474787ab941f45cfc417",
            "e8a40e2d1e7f43b488d54d5a075e4c3b",
            "53573a126edb438da1cb95ac8cd1a939"
          ]
        },
        "id": "rat_JGteKh70",
        "outputId": "fe8b206c-8728-423d-fb46-39459a32218e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e76231822f26444aab07bf685694ce92"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== r=4 fine-tuning ===\")\n",
        "trainer_r4.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "QzmO63WLKh4x",
        "outputId": "6a440423-4a9a-4edc-f94b-42a5e70704fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 63\n",
            "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 1,146,880 of 3,213,896,704 (0.04% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== r=4 fine-tuning ===\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:25, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.123900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.019600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.854800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.815800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.865900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.529300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.193000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.327300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.067000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>3.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.990500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.581600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.663800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.741200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.987100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.377500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.501400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.130500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.263100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.951000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.960400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.778500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.252800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.675900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.730300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.731000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.089700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.029900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.070600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.835500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.797000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.871200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.696800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.658500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.840100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.372400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.069700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.994600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.684600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.927100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.134300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.822900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.841900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.836600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.720200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.845200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.855200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.922900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.299700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.270300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.518900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=63, training_loss=2.436168731205047, metrics={'train_runtime': 41.1168, 'train_samples_per_second': 24.321, 'train_steps_per_second': 1.532, 'total_flos': 2092587141365760.0, 'train_loss': 2.436168731205047, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== r=4 after fine-tuning ===\")\n",
        "acc_r4_after = test_model(peft_r4, tokenizer_r4, sampled_test, pos_token_id_r4, neg_token_id_r4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDG7DGkJKh1X",
        "outputId": "6af3b540-5713-47d7-b892-3f65bf13a942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== r=4 after fine-tuning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 778.59it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  8.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 746 Total: 1000 Accuracy: 0.7460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA FINE-TUNING (r=2)"
      ],
      "metadata": {
        "id": "YEpFsegBLgis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# r=2: base model ìƒˆë¡œ ë¡œë“œ\n",
        "model_r2, tokenizer_r2 = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "0b30eb2bd11c4a568bd7ded564c7ea46",
            "c1d666f800a34a60816eed908f316357",
            "fb41b8d4192a4b578f1423d055456d41",
            "e61327c91ab84f4382c8dcda7ec9b084",
            "d1c1233db32f4300bbab8912c7a17b6d",
            "607eea14ac6a41f3886f2c910768f522",
            "543878ea3aeb4932848050b90a9e99df",
            "a796dc38053c4a61b66fe95596d3349c",
            "43fe11dce9a54c0eb417e90b927c1580",
            "13e93847c682469285b0118b408c5251",
            "24749e6e3b134a1dae60051a79d19ec4"
          ]
        },
        "id": "gqM8dwjGKhy0",
        "outputId": "2490d2dd-69da-461b-8558-19b9981a7cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.8: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b30eb2bd11c4a568bd7ded564c7ea46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bllossom/llama-3.2-Korean-Bllossom-3B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_token_id_r2 = tokenizer_r2.encode(\"ê¸ì •\", add_special_tokens=False)[0]\n",
        "neg_token_id_r2 = tokenizer_r2.encode(\"ë¶€ì •\", add_special_tokens=False)[0]"
      ],
      "metadata": {
        "id": "U1LhATEAKhtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_r2 = FastLanguageModel.get_peft_model(\n",
        "    model_r2,\n",
        "    r=2,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=True,\n",
        ")"
      ],
      "metadata": {
        "id": "EEjw4FavLra_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "trainer_r2 = SFTTrainer(\n",
        "    model=peft_r2,\n",
        "    tokenizer=tokenizer_r2,\n",
        "    train_dataset=sampled_train,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_r2\",\n",
        "        num_train_epochs=1,\n",
        "        report_to=\"none\",\n",
        "        group_by_length=False,\n",
        "    ),\n",
        "    formatting_func=formatting_prompts_func,\n",
        ")"
      ],
      "metadata": {
        "id": "ZTIu-5-jLrWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== r=2 fine-tuning ===\")\n",
        "trainer_r2.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "mLdQWiCTLrTY",
        "outputId": "2ba03568-ad11-4d34-a7e2-80e23d03fef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 63\n",
            "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 573,440 of 3,213,323,264 (0.02% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== r=2 fine-tuning ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.120800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.023300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.034600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.883700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.863600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.942600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.704800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.675900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.532500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.378300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.477500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.248600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>3.180300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.177400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.793800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.847100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.595700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.953100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.289000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.719500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.177500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.984800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.156900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.959600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.370700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.815900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.870100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.821700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.175100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.101700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.905500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.855700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.935000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.745200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.701700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.986000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.877700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.406700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.052000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.687800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.032000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.720300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.948800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.170300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.860600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.875300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.752300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.878700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.881600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.948800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.329400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.296400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.541600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=63, training_loss=2.536418665023077, metrics={'train_runtime': 24.5004, 'train_samples_per_second': 40.816, 'train_steps_per_second': 2.571, 'total_flos': 2092161603010560.0, 'train_loss': 2.536418665023077, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== r=2 after fine-tuning ===\")\n",
        "acc_r2_after = test_model(peft_r2, tokenizer_r2, sampled_test, pos_token_id_r2, neg_token_id_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMeI6_iVLrRS",
        "outputId": "96dd4f2a-f93c-4e1b-80ef-58df14806771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== r=2 after fine-tuning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 761.68it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 721 Total: 1000 Accuracy: 0.7210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEW-SHOT PROMPT INFERNECE (with r=2)"
      ],
      "metadata": {
        "id": "HVxQzWAjL591"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "import gc\n",
        "torch.cuda.empty_cache(); gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR5_Z9z6PTCl",
        "outputId": "77f79ed2-ace6-4123-cf14-d8243f4c689e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "798"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) few-shot demonstration 2ê°œ ì´ìƒ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë§Œë“¤ê¸°\n",
        "fewshot_header = \"\"\"ë‹¤ìŒì€ ì˜í™” ë¦¬ë·° ê°ì„±ë¶„ì„ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
        "ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ë§ˆì§€ë§‰ ë¦¬ë·°ì˜ ê°ì„±ì„ \"ê¸ì •\" ë˜ëŠ” \"ë¶€ì •\"ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”.\n",
        "\n",
        "[ì˜ˆì‹œ 1]\n",
        "ë¦¬ë·°: ì •ë§ ê°ë™ì ì´ê³  ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ í›Œë¥­í–ˆì–´ìš”.\n",
        "ê°ì„±: ê¸ì •\n",
        "\n",
        "[ì˜ˆì‹œ 2]\n",
        "ë¦¬ë·°: ì§€ë£¨í•˜ê³  ì „ê°œê°€ ì—‰ë§ì´ë¼ ëê¹Œì§€ ë³´ê¸° í˜ë“¤ì—ˆì–´ìš”.\n",
        "ê°ì„±: ë¶€ì •\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# 2) ê¸°ì¡´ promptë¥¼ few-shot í˜•íƒœë¡œ êµì²´\n",
        "prompt_backup = prompt  # ì›ë˜ zero-shot prompt ë°±ì—…\n",
        "\n",
        "prompt = fewshot_header + \"\"\"[ë¬¸ì œ]\n",
        "ë¦¬ë·°: {}\n",
        "\n",
        "ê°ì„±: {}\"\"\"\n",
        "\n",
        "print(\"=== LoRA(r=2) few-shot ===\")\n",
        "fewshot_acc = test_model(peft_r2, tokenizer_r2, sampled_test, pos_token_id_r2, neg_token_id_r2)\n",
        "\n",
        "prompt = prompt_backup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHPd7vEpLxyc",
        "outputId": "73714b11-62fd-4142-f4ad-9e464b95e705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LoRA(r=2) few-shot ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 690.69it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:13<00:00,  7.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 624 Total: 1000 Accuracy: 0.6240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REASONING ë°ì´í„°ì…‹ ìƒì„± ë° í•™ìŠµ"
      ],
      "metadata": {
        "id": "4gzsDUy3QocX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Imports & Logging\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time, json, random, re, math\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"tornado.access\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "HtGWWhik03hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Gemini API ì„¤ì •\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model_gemini = genai.GenerativeModel(\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "0j9MLBq003e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) ìœ í‹¸: JSON ë°°ì—´ ì¶”ì¶œ + \"1ë¬¸ì¥\" ê²€ì¦\n",
        "\n",
        "def _extract_json_array(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    start = text.find(\"[\")\n",
        "    end   = text.rfind(\"]\")\n",
        "    if start != -1 and end != -1:\n",
        "        text = text[start:end+1]\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _is_single_sentence_reason(s: str) -> bool:\n",
        "\n",
        "    if s is None:\n",
        "        return False\n",
        "    s = str(s).strip()\n",
        "    if not s:\n",
        "        return False\n",
        "    if \"\\n\" in s or \"\\r\" in s:\n",
        "        return False\n",
        "\n",
        "    end_ok = bool(re.search(r\"[\\.!?â€¦]$\", s))\n",
        "    if not end_ok:\n",
        "        return False\n",
        "\n",
        "    cnt = len(re.findall(r\"[\\.!?â€¦]\", s))\n",
        "    if cnt != 1:\n",
        "        return False\n",
        "\n",
        "    if len(s) < 6:\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "OT9BOkAi03b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) ë°°ì¹˜ reasoning ìƒì„±\n",
        "\n",
        "def generate_reasoning_batch(reviews, labels, max_retries=2):\n",
        "\n",
        "    N = len(reviews)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "ë„ˆëŠ” ë°ì´í„° ë¼ë²¨ë§ ì „ë¬¸ê°€ì•¼.\n",
        "ê° ë¦¬ë·°ì˜ ì •ë‹µ ê°ì„±(ê¸ì •/ë¶€ì •)ì„ ì°¸ê³ í•´ì„œ, ê·¼ê±°(reason)ë¥¼ í•œêµ­ì–´ 'ë”± 1ë¬¸ì¥'ìœ¼ë¡œë§Œ ì‘ì„±í•´.\n",
        "\n",
        "ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì„ ì§€ì¼œ:\n",
        "- ì¶œë ¥ì€ JSON ë°°ì—´ë§Œ. (ì„¤ëª…/í…ìŠ¤íŠ¸/ì½”ë“œíœìŠ¤ ì ˆëŒ€ ê¸ˆì§€)\n",
        "- ë°°ì—´ ê¸¸ì´ëŠ” ë°˜ë“œì‹œ {N}ê°œ\n",
        "- ê° ì›ì†ŒëŠ” ë°˜ë“œì‹œ {{\"reason\":\"...\"}} í˜•íƒœ\n",
        "- reasonì€ ë¹ˆ ë¬¸ìì—´ ê¸ˆì§€\n",
        "- reasonì€ ì¤„ë°”ê¿ˆ ê¸ˆì§€\n",
        "- reasonì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œ(.) ë˜ëŠ” ! ë˜ëŠ” ? ë˜ëŠ” â€¦ ì¤‘ í•˜ë‚˜ë¡œ \"ëë‚˜ì•¼\" í•¨\n",
        "- ê·¸ë¦¬ê³  ìœ„ ì¢…ê²°ë¶€í˜¸ëŠ” reason ì „ì²´ì—ì„œ \"ì •í™•íˆ 1ë²ˆë§Œ\" ë“±ì¥í•´ì•¼ í•¨ (ë‘ ë¬¸ì¥ ê¸ˆì§€)\n",
        "- ì…ë ¥ ìˆœì„œ ê·¸ëŒ€ë¡œ ì¶œë ¥\n",
        "\n",
        "ì¶œë ¥ ì˜ˆì‹œ(í˜•ì‹ë§Œ ì°¸ê³ ):\n",
        "[\n",
        "  {{\"reason\":\"ê°ë™ì ì´ê³  ì¬ë¯¸ìˆë‹¤ëŠ” í‘œí˜„ì´ ìˆì–´ ê¸ì •ìœ¼ë¡œ íŒë‹¨í–ˆë‹¤.\"}},\n",
        "  {{\"reason\":\"ì§€ë£¨í•˜ê³  ë³„ë¡œë¼ëŠ” í‰ê°€ê°€ ìˆì–´ ë¶€ì •ìœ¼ë¡œ íŒë‹¨í–ˆë‹¤.\"}}\n",
        "]\n",
        "\"\"\".strip()\n",
        "\n",
        "    for r, l in zip(reviews, labels):\n",
        "        label_str = \"ê¸ì •\" if int(l) == 1 else \"ë¶€ì •\"\n",
        "        r = str(r).replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()[:120]\n",
        "        prompt += f'\\n\\në¦¬ë·°: \"{r}\"\\nì •ë‹µ: {label_str}'\n",
        "\n",
        "    max_out = min(4096, 120 * N + 200)\n",
        "\n",
        "    gen_cfg = {\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_output_tokens\": max_out,\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    spent_calls = 0\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            spent_calls += 1\n",
        "            try:\n",
        "                resp = model_gemini.generate_content(prompt, generation_config=gen_cfg)\n",
        "            except TypeError:\n",
        "\n",
        "                gen_cfg.pop(\"response_mime_type\", None)\n",
        "                raise\n",
        "\n",
        "            data = _extract_json_array(resp.text)\n",
        "\n",
        "\n",
        "            reasons = [None] * N\n",
        "\n",
        "            if isinstance(data, list):\n",
        "\n",
        "                for i in range(min(N, len(data))):\n",
        "                    if isinstance(data[i], dict):\n",
        "                        reason = str(data[i].get(\"reason\", \"\")).strip()\n",
        "                        if reason:\n",
        "                            reasons[i] = reason\n",
        "\n",
        "            raw = (resp.text or \"\").strip()\n",
        "            for i in range(N):\n",
        "                if reasons[i] is None:\n",
        "                    reasons[i] = raw\n",
        "\n",
        "            cleaned = [{\"reason\": r} for r in reasons]\n",
        "            return cleaned, spent_calls\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            msg = str(e)\n",
        "\n",
        "            if \"429\" in msg or \"Too Many Requests\" in msg:\n",
        "                wait = min(60, 8 * (2 ** attempt)) + random.uniform(0, 2)\n",
        "                print(f\"[429] Backoff {wait:.1f}s\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                time.sleep(1.5)\n",
        "\n",
        "    return [], spent_calls\n"
      ],
      "metadata": {
        "id": "BzvLaZg103Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) NSMC 1000ê°œ ìƒ˜í”Œë§\n",
        "\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "\n",
        "dataset = load_dataset(\"Blpeng/nsmc\")\n",
        "\n",
        "SAMPLED_N = 1000\n",
        "SAMPLED_TEST_N = 1000\n",
        "\n",
        "sampled_train = dataset[\"train\"].shuffle(seed=3407).select(range(SAMPLED_N))\n",
        "sampled_test  = dataset[\"test\"].shuffle(seed=3407).select(range(SAMPLED_TEST_N))\n",
        "\n",
        "reviews = list(sampled_train[\"document\"])\n",
        "labels  = list(sampled_train[\"label\"])\n",
        "\n",
        "print(\"Train sampled:\", len(reviews))\n",
        "print(\"Test sampled :\", len(sampled_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkCJUIYw05ec",
        "outputId": "b667459e-d3ad-454f-cb6a-60356817380f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sampled: 1000\n",
            "Test sampled : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) ë¹„ìš© ë°©ì§€í˜• ìƒì„± ë£¨í”„\n",
        "\n",
        "MAX_API_CALLS_TOTAL   = 600\n",
        "MAX_RETRIES_PER_CALL  = 2\n",
        "MAX_SPLIT_DEPTH       = 2\n",
        "ALLOW_SKIP            = True\n",
        "\n",
        "batch_size            = 20\n",
        "MIN_SECONDS_PER_CALL  = 0.2\n",
        "\n",
        "api_calls_used = 0\n",
        "new_data = []\n",
        "failed_items = []\n",
        "\n",
        "pbar = tqdm(total=len(reviews), desc=\"Generating 1-sentence reasoning (API)\")\n",
        "\n",
        "def solve_segment(start_idx, seg_reviews, seg_labels, depth=0):\n",
        "\n",
        "    global api_calls_used, new_data, failed_items\n",
        "\n",
        "    if api_calls_used >= MAX_API_CALLS_TOTAL:\n",
        "        return False\n",
        "\n",
        "    results, spent = generate_reasoning_batch(seg_reviews, seg_labels, max_retries=MAX_RETRIES_PER_CALL)\n",
        "    api_calls_used += spent\n",
        "\n",
        "\n",
        "    if isinstance(results, list) and len(results) == len(seg_reviews):\n",
        "        for j, r in enumerate(seg_reviews):\n",
        "            gt = int(seg_labels[j])\n",
        "            gt_str = \"ê¸ì •\" if gt == 1 else \"ë¶€ì •\"\n",
        "            reason = str(results[j][\"reason\"]).strip()\n",
        "\n",
        "\n",
        "            new_data.append({\"review\": r, \"label\": gt_str, \"reason\": reason})\n",
        "\n",
        "\n",
        "\n",
        "        pbar.update(len(seg_reviews))\n",
        "        time.sleep(MIN_SECONDS_PER_CALL)\n",
        "        return True\n",
        "\n",
        "\n",
        "    if depth < MAX_SPLIT_DEPTH and len(seg_reviews) > 1:\n",
        "        mid = len(seg_reviews) // 2\n",
        "        left_ok  = solve_segment(start_idx, seg_reviews[:mid], seg_labels[:mid], depth+1)\n",
        "        right_ok = solve_segment(start_idx+mid, seg_reviews[mid:], seg_labels[mid:], depth+1)\n",
        "        return left_ok and right_ok\n",
        "\n",
        "\n",
        "    if ALLOW_SKIP:\n",
        "        for j in range(len(seg_reviews)):\n",
        "            failed_items.append((start_idx + j, int(seg_labels[j]), \"final_fail\"))\n",
        "        pbar.update(len(seg_reviews))\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "i = 0\n",
        "while i < len(reviews):\n",
        "    if api_calls_used >= MAX_API_CALLS_TOTAL:\n",
        "        print(f\"\\n[STOP] cost cap reached: api_calls_used={api_calls_used}\")\n",
        "        break\n",
        "\n",
        "    bs = min(batch_size, len(reviews) - i)\n",
        "    seg_r = reviews[i:i+bs]\n",
        "    seg_l = labels[i:i+bs]\n",
        "\n",
        "    ok = solve_segment(i, seg_r, seg_l, depth=0)\n",
        "    if ok:\n",
        "        i += bs\n",
        "    else:\n",
        "        print(f\"\\n[STOP] hard fail at i={i}, api_calls_used={api_calls_used}\")\n",
        "        break\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "df_cot = pd.DataFrame(new_data)\n",
        "\n",
        "print(\"\\n=== Generation Summary ===\")\n",
        "print(\"api_calls_used:\", api_calls_used)\n",
        "print(\"generated rows:\", len(df_cot), \"/\", len(reviews))\n",
        "print(\"failed items :\", len(failed_items))\n",
        "empty_ratio = (df_cot[\"reason\"].fillna(\"\").str.strip() == \"\").mean() if len(df_cot) else 1.0\n",
        "print(\"EMPTY reason ratio:\", empty_ratio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "4WUUPKzj05Zw",
        "outputId": "c996446f-33e3-4637-8e17-ca89f33336e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-sentence reasoning (API): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [10:32<00:00,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generation Summary ===\n",
            "api_calls_used: 50\n",
            "generated rows: 1000 / 1000\n",
            "failed items : 0\n",
            "EMPTY reason ratio: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) SFTìš© ë°ì´í„° êµ¬ì„± : ë¬¸ì œ-ì •ë‹µ-ê·¼ê±°\n",
        "\n",
        "reasoning_prompt_train = \"\"\"ë¬¸ì œ: {review}\n",
        "ì •ë‹µ: {label}\n",
        "ê·¼ê±°: {reason}\"\"\"\n",
        "\n",
        "def formatting_prompts_reasoning(batch):\n",
        "    if isinstance(batch[\"review\"], str):\n",
        "        return [reasoning_prompt_train.format(\n",
        "            review=batch[\"review\"],\n",
        "            label=batch[\"label\"],\n",
        "            reason=batch[\"reason\"],\n",
        "        )]\n",
        "    return [\n",
        "        reasoning_prompt_train.format(review=r, label=l, reason=rs)\n",
        "        for r, l, rs in zip(batch[\"review\"], batch[\"label\"], batch[\"reason\"])\n",
        "    ]\n",
        "\n",
        "train_reason_ds = HFDataset.from_pandas(df_cot.reset_index(drop=True))\n",
        "\n",
        "print(\"\\n=== train_reason_ds sample ===\")\n",
        "print(train_reason_ds[0])\n",
        "print(\"\\n=== SFT input sample ===\")\n",
        "print(reasoning_prompt_train.format(\n",
        "    review=train_reason_ds[0][\"review\"],\n",
        "    label=train_reason_ds[0][\"label\"],\n",
        "    reason=train_reason_ds[0][\"reason\"],\n",
        "))\n"
      ],
      "metadata": {
        "id": "jiXmNLjwy6is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dae1448-ddf3-4773-9628-bf314e4916a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== train_reason_ds sample ===\n",
            "{'review': 'ì €ëŠ” ì˜¤íˆë ¤ ê²°ë§ì´ ì¢‹ì•˜ë‹¤ê³  ëŠê¼ˆëŠ”ë° ë³´ì—¬ì£¼ì§„ ì•Šì•˜ì§€ë§Œ ì €ëŠ” ë§Œë‚¬ì„ ê±°ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì”ì”í•œ ì—¬ìš´ê³¼ ì“¸ì“¸í•¨ì´ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¸ë„ì˜ ë¬¸í™”ë‚˜ ìŒì‹ë“¤ì„ ë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ì–´ìš”.', 'label': 'ê¸ì •', 'reason': 'ê²°ë§ì´ ì¢‹ì•˜ê³  ì¸ë„ì˜ ë¬¸í™”ì™€ ìŒì‹ì„ ë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤ëŠ” í‘œí˜„ì—ì„œ ê¸ì •ì„ íŒë‹¨í–ˆë‹¤.'}\n",
            "\n",
            "=== SFT input sample ===\n",
            "ë¬¸ì œ: ì €ëŠ” ì˜¤íˆë ¤ ê²°ë§ì´ ì¢‹ì•˜ë‹¤ê³  ëŠê¼ˆëŠ”ë° ë³´ì—¬ì£¼ì§„ ì•Šì•˜ì§€ë§Œ ì €ëŠ” ë§Œë‚¬ì„ ê±°ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì”ì”í•œ ì—¬ìš´ê³¼ ì“¸ì“¸í•¨ì´ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¸ë„ì˜ ë¬¸í™”ë‚˜ ìŒì‹ë“¤ì„ ë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ì–´ìš”.\n",
            "ì •ë‹µ: ê¸ì •\n",
            "ê·¼ê±°: ê²°ë§ì´ ì¢‹ì•˜ê³  ì¸ë„ì˜ ë¬¸í™”ì™€ ìŒì‹ì„ ë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤ëŠ” í‘œí˜„ì—ì„œ ê¸ì •ì„ íŒë‹¨í–ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) SFT í•™ìŠµ + ì„±ëŠ¥ í™•ì¸\n",
        "\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"\\n=== BEFORE reasoning SFT ===\")\n",
        "_ = test_model(peft_r2, tokenizer_r2, sampled_test, pos_token_id_r2, neg_token_id_r2)\n",
        "\n",
        "trainer_reason = SFTTrainer(\n",
        "    model=peft_r2,\n",
        "    tokenizer=tokenizer_r2,\n",
        "    train_dataset=train_reason_ds,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_r2_reasoning\",\n",
        "        num_train_epochs=1,\n",
        "        report_to=\"none\",\n",
        "        group_by_length=False,\n",
        "    ),\n",
        "    formatting_func=formatting_prompts_reasoning,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "LzE7fqrozmCK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "a1953cbed85749bf9978762615365178",
            "7846968737ab4badad31a399704b0596",
            "485b5e5b495a4f74b8786e5b9323dba9",
            "5561263962dc4df59c5c739d41797742",
            "c43e74a07a114f06b4d6e77b04ac2f18",
            "8eff09e6b59b4487bee8c21b535ee482",
            "efa992a025f948ce967e6f5fdfa39550",
            "fbf0e6114f3c4321935cbcabaecfcb65",
            "ad6b86ce98bb42af87f3ae92d060b22b",
            "a453fc3a74fb4db983ea697a670ff515",
            "5b0a20ad6e9b41718d84d4a4abe91083"
          ]
        },
        "outputId": "c25b9336-fa4f-4c46-8ab9-0b5315155c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BEFORE reasoning SFT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 791.31it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  8.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 721 Total: 1000 Accuracy: 0.7210\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1953cbed85749bf9978762615365178"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== TRAIN reasoning SFT ===\")\n",
        "trainer_reason.train()"
      ],
      "metadata": {
        "id": "UkP-T6MPzl_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "7ce6e52f-ad4a-4043-e6f7-2c6df534564d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 63\n",
            "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 573,440 of 3,213,323,264 (0.02% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAIN reasoning SFT ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 01:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.754700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.404800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.303400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.120500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=63, training_loss=2.2937850043887185, metrics={'train_runtime': 86.6063, 'train_samples_per_second': 11.546, 'train_steps_per_second': 0.727, 'total_flos': 1.209894619127808e+16, 'train_loss': 2.2937850043887185, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== AFTER reasoning SFT ===\")\n",
        "_ = test_model(peft_r2, tokenizer_r2, sampled_test, pos_token_id_r2, neg_token_id_r2)"
      ],
      "metadata": {
        "id": "7Dbk5V-ezl8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b73b884-be10-4229-cb52-4c3d97aa6f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER reasoning SFT ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 774.59it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  8.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 709 Total: 1000 Accuracy: 0.7090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}